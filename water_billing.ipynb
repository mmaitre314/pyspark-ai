{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo of water utility billing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A note on clocks. The system deals with 3 groups of clocks, which are allowed to drift by at-most 5 minutes of each other:\n",
    "- meter clocks (one clock per device)\n",
    "- ingestion clocks (one clock per ingestion machine)\n",
    "- job-trigger clock (single clock)\n",
    "\n",
    "Meter and ingestion clocks give wall-clock time. Trigger clock also typically gives wall-clock time, except during backfills and reprocessing where it may be days in the past."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Parameters - These can be overridden when running programmatically\n",
    "\n",
    "# Tumbling window parameters for batch processing\n",
    "# WINDOW_END_TIME must be at least 5 minutes less than wall-clock time to avoid clock-drift issues\n",
    "# between the orchestrator and data sources. This ensures late-arriving data within the drift\n",
    "# tolerance is not accidentally excluded.\n",
    "WINDOW_START_TIME = '2026-01-15T08:00:00'  # ISO format: YYYY-MM-DDTHH:MM:SS\n",
    "WINDOW_END_TIME = '2026-01-15T14:00:00'    # ISO format: YYYY-MM-DDTHH:MM:SS\n",
    "\n",
    "INPUT_PATH = 'data/input/raw'\n",
    "HOURLY_OUTPUT_PATH = 'data/output/hourly_usage'\n",
    "CUMULATIVE_OUTPUT_PATH = 'data/output/cumulative_usage'\n",
    "DEDUPLICATED_RAW_PATH = 'data/output/deduplicated_raw'\n",
    "VERBOSE = True  # Set VERBOSE to False to skip display-only cells (useful for testing)\n",
    "\n",
    "spark = None  # Set to an existing SparkSession to use it instead of creating a new one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark session if not provided externally\n",
    "# When running tests, spark session is passed via the spark parameter\n",
    "if spark is None:\n",
    "    from spark_utils import create_spark_session\n",
    "    spark = create_spark_session(app_name='WaterBilling', log_level='WARN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle raw-data duplicates and out-of-order records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "from schemas import USAGE_INPUT_SCHEMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input data as batch source for the tumbling window\n",
    "# Data is stored in Hive-style partitioned directories: INPUT_PATH/ingestion_hour=yyyy-MM-dd-HH/\n",
    "# Spark partition discovery automatically adds ingestion_hour as a column\n",
    "# WINDOW_START_TIME is inclusive, WINDOW_END_TIME is exclusive for partition selection\n",
    "\n",
    "# Parse window parameters\n",
    "window_start = datetime.fromisoformat(WINDOW_START_TIME)\n",
    "window_end = datetime.fromisoformat(WINDOW_END_TIME)\n",
    "\n",
    "# Format window boundaries for partition filtering (matches Hive partition format)\n",
    "window_start_partition = window_start.strftime('%Y-%m-%d-%H')\n",
    "window_end_partition = window_end.strftime('%Y-%m-%d-%H')\n",
    "\n",
    "# Read with partition discovery - Spark automatically discovers ingestion_hour partitions\n",
    "# and adds ingestion_hour as a string column to the DataFrame\n",
    "if os.path.isdir(INPUT_PATH):\n",
    "    raw_batch = (\n",
    "        spark.read\n",
    "        .format('json')\n",
    "        .schema(USAGE_INPUT_SCHEMA)\n",
    "        .option('basePath', INPUT_PATH)\n",
    "        .load(f'{INPUT_PATH}/ingestion_hour=*')\n",
    "        # Filter partitions: start is inclusive, end is exclusive\n",
    "        .filter(\n",
    "            (F.col('ingestion_hour') >= window_start_partition) &\n",
    "            (F.col('ingestion_hour') < window_end_partition)\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    # No data directory - create empty DataFrame with schema plus partition column\n",
    "    raw_batch = (\n",
    "        spark.createDataFrame([], USAGE_INPUT_SCHEMA)\n",
    "        .withColumn('ingestion_hour', F.lit(None).cast('string'))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter records based on watermark logic (similar to streaming watermark)\n",
    "# Watermark is computed relative to the partition's ingestion_hour, not window_end\n",
    "# This allows each partition to have its own 7-day late data tolerance\n",
    "#\n",
    "# Clock drift tolerance: Allow recording_time to be up to 5 minutes beyond the partition\n",
    "# end time. This handles cases where the meter clock is slightly ahead of the ingestion system.\n",
    "\n",
    "# Convert ingestion_hour string partition column to timestamp for filtering\n",
    "raw_with_ingestion = (\n",
    "    raw_batch\n",
    "    .withColumn('ingestion_time', \n",
    "        F.to_timestamp(F.col('ingestion_hour'), 'yyyy-MM-dd-HH'))\n",
    ")\n",
    "\n",
    "# Compute watermark cutoff: 7 days before the partition's ingestion hour\n",
    "# Compute clock drift allowance: 5 minutes after the partition's hour ends\n",
    "filtered_batch = (\n",
    "    raw_with_ingestion\n",
    "    .filter(\n",
    "        # Exclude records older than 7 days relative to their ingestion partition\n",
    "        (F.col('recording_time') >= F.col('ingestion_time') - F.expr('INTERVAL 7 DAYS')) &\n",
    "        # Allow recording_time up to 5 minutes after partition hour ends (clock drift tolerance)\n",
    "        (F.col('recording_time') <= F.col('ingestion_time') + F.expr('INTERVAL 1 HOUR') + F.expr('INTERVAL 5 MINUTES'))\n",
    "    )\n",
    "    .drop('ingestion_hour', 'ingestion_time')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add partition column for the deduplicated raw table\n",
    "# Partitioning by recording hour improves merge performance and data organization\n",
    "filtered_with_partition = (\n",
    "    filtered_batch\n",
    "    .withColumn('recording_hour', F.date_format(F.date_trunc('hour', F.col('recording_time')), 'yyyy-MM-dd-HH'))\n",
    ")\n",
    "\n",
    "# Track min/max recording_hour for efficient partition pruning when reading deduplicated data\n",
    "# This avoids reading all historical partitions during hourly aggregation\n",
    "recording_hour_range = filtered_with_partition.agg(\n",
    "    F.min('recording_hour').alias('min_hour'),\n",
    "    F.max('recording_hour').alias('max_hour')\n",
    ").collect()[0]\n",
    "min_recording_hour = recording_hour_range['min_hour']\n",
    "max_recording_hour = recording_hour_range['max_hour']\n",
    "print(f'Recording hour range: {min_recording_hour} to {max_recording_hour}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge current window's records into intermediate Delta table\n",
    "# Deduplication is handled in two stages:\n",
    "# 1. dropDuplicates() removes duplicates within the current batch\n",
    "# 2. Delta merge with whenNotMatchedInsertAll() prevents cross-batch duplicates\n",
    "# Table is partitioned by recording_hour for efficient querying and merges.\n",
    "\n",
    "# Deduplicate within the current batch first (required by Delta merge)\n",
    "deduplicated_df = filtered_with_partition.dropDuplicates(['record_id'])\n",
    "\n",
    "if DeltaTable.isDeltaTable(spark, DEDUPLICATED_RAW_PATH):\n",
    "    delta_table = DeltaTable.forPath(spark, DEDUPLICATED_RAW_PATH)\n",
    "    \n",
    "    # Merge: insert only if record_id doesn't exist (idempotent deduplication)\n",
    "    # Include recording_hour in condition to enable partition pruning\n",
    "    delta_table.alias('target').merge(\n",
    "        deduplicated_df.alias('source'),\n",
    "        'target.recording_hour = source.recording_hour AND target.record_id = source.record_id'\n",
    "    ).whenNotMatchedInsertAll().execute()\n",
    "    \n",
    "    print('Merged records into deduplicated raw table')\n",
    "else:\n",
    "    # First write - create the table partitioned by recording_hour\n",
    "    (deduplicated_df.write\n",
    "        .format('delta')\n",
    "        .partitionBy('recording_hour')\n",
    "        .mode('overwrite')\n",
    "        .save(DEDUPLICATED_RAW_PATH))\n",
    "    print(f'Created deduplicated raw table with {deduplicated_df.count()} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute hourly usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read only the affected partitions from deduplicated data for downstream processing\n",
    "# Uses min/max recording_hour tracked earlier for efficient partition pruning\n",
    "if min_recording_hour is not None:\n",
    "    all_deduplicated = (\n",
    "        spark.read.format('delta').load(DEDUPLICATED_RAW_PATH)\n",
    "        .filter(\n",
    "            (F.col('recording_hour') >= min_recording_hour) &\n",
    "            (F.col('recording_hour') <= max_recording_hour)\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    # No data in current window - create empty DataFrame\n",
    "    all_deduplicated = spark.read.format('delta').load(DEDUPLICATED_RAW_PATH).limit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate usage by customer and hour\n",
    "# Keep the partition column for writing\n",
    "hourly_aggregated = (\n",
    "    all_deduplicated\n",
    "    .groupBy('customer_id', 'recording_hour')\n",
    "    .agg(F.sum('usage_gallons').alias('total_usage_gallons'))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write hourly aggregated data using partition overwrite\n",
    "# Since we recompute complete aggregations for [min_hour, max_hour], we can use replaceWhere\n",
    "# to atomically replace those partitions. This is simpler and more efficient than merge.\n",
    "(hourly_aggregated.write\n",
    "    .format('delta')\n",
    "    .mode('overwrite')\n",
    "    .option('replaceWhere', f\"recording_hour >= '{min_recording_hour}' AND recording_hour <= '{max_recording_hour}'\")\n",
    "    .save(HOURLY_OUTPUT_PATH))\n",
    "\n",
    "print(f'Replaced hourly partitions from {min_recording_hour} to {max_recording_hour}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute cumulative monthly usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the upper bound for cumulative recomputation\n",
    "# Use the greater of: max_recording_hour from current batch, or max existing in cumulative table\n",
    "if DeltaTable.isDeltaTable(spark, CUMULATIVE_OUTPUT_PATH):\n",
    "    existing_max = (\n",
    "        spark.read.format('delta').load(CUMULATIVE_OUTPUT_PATH)\n",
    "        .agg(F.max('recording_hour'))\n",
    "        .collect()[0][0]\n",
    "    )\n",
    "    if existing_max and max_recording_hour:\n",
    "        cumulative_max_hour = max(max_recording_hour, existing_max)\n",
    "    else:\n",
    "        cumulative_max_hour = max_recording_hour or existing_max\n",
    "else:\n",
    "    cumulative_max_hour = max_recording_hour\n",
    "\n",
    "print(f'Cumulative recomputation range: {min_recording_hour} to {cumulative_max_hour}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+-------------------+\n",
      "|customer_id  |usage_hour         |total_usage_gallons|\n",
      "+-------------+-------------------+-------------------+\n",
      "|customer_0000|2026-01-15 08:00:00|63.02999999999999  |\n",
      "|customer_0000|2026-01-15 09:00:00|59.129999999999995 |\n",
      "|customer_0000|2026-01-15 10:00:00|54.440000000000005 |\n",
      "|customer_0000|2026-01-15 11:00:00|60.93000000000001  |\n",
      "|customer_0001|2026-01-15 08:00:00|61.61              |\n",
      "|customer_0001|2026-01-15 09:00:00|61.40999999999999  |\n",
      "|customer_0001|2026-01-15 10:00:00|64.83999999999999  |\n",
      "|customer_0001|2026-01-15 11:00:00|65.60000000000001  |\n",
      "|customer_0002|2026-01-15 08:00:00|54.68000000000001  |\n",
      "|customer_0002|2026-01-15 09:00:00|80.36999999999999  |\n",
      "+-------------+-------------------+-------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Read hourly data with partition pruning - only load partitions that may need recomputation\n",
    "# This includes min_recording_hour to cumulative_max_hour range\n",
    "if min_recording_hour is not None and cumulative_max_hour is not None:\n",
    "    hourly_usage_df = (\n",
    "        spark.read.format('delta').load(HOURLY_OUTPUT_PATH)\n",
    "        .filter(\n",
    "            (F.col('recording_hour') >= min_recording_hour) &\n",
    "            (F.col('recording_hour') <= cumulative_max_hour)\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    hourly_usage_df = spark.read.format('delta').load(HOURLY_OUTPUT_PATH).limit(0)\n",
    "\n",
    "if VERBOSE:\n",
    "    hourly_usage_df.orderBy('customer_id', 'recording_hour').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No batch data tracked, using watermark fallback: 2026-01-19 01:00:00\n",
      "Cumulative update window starts at: 2026-01-19 01:00:00\n",
      "Affected months: []\n",
      "Cumulative records to update: 0\n"
     ]
    }
   ],
   "source": [
    "# Compute cumulative monthly usage\n",
    "# Recomputes cumulative totals for all hours from min_recording_hour to cumulative_max_hour\n",
    "# Cumulative sum resets at the start of each month (partitioned by customer and month)\n",
    "\n",
    "if min_recording_hour is None:\n",
    "    print('No data to process for cumulative computation')\n",
    "    cumulative_usage_df = spark.createDataFrame([], 'customer_id STRING, recording_hour STRING, cumulative_usage_gallons DOUBLE')\n",
    "else:\n",
    "    # Get affected months based on the recording_hour range being processed\n",
    "    affected_months = (\n",
    "        hourly_usage_df\n",
    "        .withColumn('recording_time_ts', F.to_timestamp(F.col('recording_hour'), 'yyyy-MM-dd-HH'))\n",
    "        .select(F.date_trunc('month', F.col('recording_time_ts')).alias('month'))\n",
    "        .distinct()\n",
    "        .collect()\n",
    "    )\n",
    "    affected_month_values = [row['month'] for row in affected_months]\n",
    "    print(f'Affected months: {affected_month_values}')\n",
    "    \n",
    "    # For correct cumulative computation, we need all hourly data from the START of each affected month\n",
    "    # This ensures running totals are computed correctly even if min_recording_hour is mid-month\n",
    "    # Read full months for affected periods\n",
    "    hourly_full_months = (\n",
    "        spark.read.format('delta').load(HOURLY_OUTPUT_PATH)\n",
    "        .withColumn('recording_time_ts', F.to_timestamp(F.col('recording_hour'), 'yyyy-MM-dd-HH'))\n",
    "        .filter(F.date_trunc('month', F.col('recording_time_ts')).isin(affected_month_values))\n",
    "        # Only include up to cumulative_max_hour\n",
    "        .filter(F.col('recording_hour') <= cumulative_max_hour)\n",
    "        .withColumn('usage_month', F.date_trunc('month', F.col('recording_time_ts')))\n",
    "    )\n",
    "    \n",
    "    # Define window for cumulative sum: partition by customer and month, order by hour\n",
    "    # This ensures cumulative sum resets at month boundaries\n",
    "    cumulative_window = (\n",
    "        Window\n",
    "        .partitionBy('customer_id', 'usage_month')\n",
    "        .orderBy('recording_hour')\n",
    "        .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "    )\n",
    "    \n",
    "    # Compute cumulative usage for affected months\n",
    "    cumulative_all = (\n",
    "        hourly_full_months\n",
    "        .withColumn('cumulative_usage_gallons', F.sum('total_usage_gallons').over(cumulative_window))\n",
    "        .select('customer_id', 'recording_hour', 'cumulative_usage_gallons')\n",
    "    )\n",
    "    \n",
    "    # Only keep records >= min_recording_hour (these are the ones that need updating)\n",
    "    # Earlier records in the month are needed for window computation but don't need updating\n",
    "    cumulative_usage_df = cumulative_all.filter(F.col('recording_hour') >= min_recording_hour)\n",
    "    \n",
    "    print(f'Cumulative records to update: {cumulative_usage_df.count()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No cumulative records to update\n"
     ]
    }
   ],
   "source": [
    "# Write cumulative data using partition overwrite\n",
    "# Since we recompute complete cumulative values for hours >= min_recording_hour,\n",
    "# we can use replaceWhere to atomically replace those partitions.\n",
    "(cumulative_usage_df.write\n",
    "    .format('delta')\n",
    "    .mode('overwrite')\n",
    "    .option('replaceWhere', f\"recording_hour >= '{min_recording_hour}'\")\n",
    "    .save(CUMULATIVE_OUTPUT_PATH))\n",
    "\n",
    "print(f'Replaced cumulative partitions from {min_recording_hour} onwards')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------------------+------------------------+\n",
      "|customer_id  |usage_hour         |usage_hour_partition|cumulative_usage_gallons|\n",
      "+-------------+-------------------+--------------------+------------------------+\n",
      "|customer_0000|2026-01-15 08:00:00|2026-01-15-08       |63.02999999999999       |\n",
      "|customer_0000|2026-01-15 09:00:00|2026-01-15-09       |122.15999999999998      |\n",
      "|customer_0000|2026-01-15 10:00:00|2026-01-15-10       |176.6                   |\n",
      "|customer_0000|2026-01-15 11:00:00|2026-01-15-11       |237.53                  |\n",
      "|customer_0000|2026-01-15 12:00:00|2026-01-15-12       |303.51                  |\n",
      "|customer_0000|2026-01-15 13:00:00|2026-01-15-13       |363.52                  |\n",
      "|customer_0001|2026-01-15 08:00:00|2026-01-15-08       |61.61                   |\n",
      "|customer_0001|2026-01-15 09:00:00|2026-01-15-09       |123.01999999999998      |\n",
      "|customer_0001|2026-01-15 10:00:00|2026-01-15-10       |187.85999999999996      |\n",
      "|customer_0001|2026-01-15 11:00:00|2026-01-15-11       |253.45999999999998      |\n",
      "+-------------+-------------------+--------------------+------------------------+\n",
      "only showing top 10 rows\n"
     ]
    }
   ],
   "source": [
    "# Read and display cumulative usage\n",
    "cumulative_df = spark.read.format('delta').load(CUMULATIVE_OUTPUT_PATH)\n",
    "if VERBOSE:\n",
    "    cumulative_df.orderBy('customer_id', 'recording_hour').show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hourly Usage Summary ===\n",
      "+-------+-------------------+\n",
      "|summary|total_usage_gallons|\n",
      "+-------+-------------------+\n",
      "|  count|                 20|\n",
      "|   mean| 63.708499999999994|\n",
      "| stddev|  6.900069622074381|\n",
      "|    min|  51.28999999999999|\n",
      "|    max|  80.36999999999999|\n",
      "+-------+-------------------+\n",
      "\n",
      "=== Cumulative Usage Summary ===\n",
      "+-------+------------------------+\n",
      "|summary|cumulative_usage_gallons|\n",
      "+-------+------------------------+\n",
      "|  count|                      20|\n",
      "|   mean|      157.79900000000004|\n",
      "| stddev|       75.20760256222702|\n",
      "|    min|       51.28999999999999|\n",
      "|    max|                  270.73|\n",
      "+-------+------------------------+\n",
      "\n",
      "Total hourly records: 20\n",
      "Total cumulative records: 20\n",
      "Unique customers: 5\n"
     ]
    }
   ],
   "source": [
    "# Summary statistics (only when VERBOSE is enabled)\n",
    "if VERBOSE:\n",
    "    print('=== Hourly Usage Summary ===')\n",
    "    hourly_usage_df.describe('total_usage_gallons').show()\n",
    "\n",
    "    print('=== Cumulative Usage Summary ===')\n",
    "    cumulative_df.describe('cumulative_usage_gallons').show()\n",
    "\n",
    "    print(f'Total hourly records: {hourly_usage_df.count()}')\n",
    "    print(f'Total cumulative records: {cumulative_df.count()}')\n",
    "    print(f\"Unique customers: {hourly_usage_df.select('customer_id').distinct().count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Hourly Usage Delta Table Structure ===\n",
      "total 0\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 .\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 ..\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 _delta_log\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 usage_hour_partition=2026-01-15-08\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 usage_hour_partition=2026-01-15-09\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 usage_hour_partition=2026-01-15-10\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:24 usage_hour_partition=2026-01-15-11\n",
      "\n",
      "\n",
      "=== Cumulative Usage Delta Table Structure ===\n",
      "total 0\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 .\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 ..\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 _delta_log\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 usage_hour_partition=2026-01-15-08\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 usage_hour_partition=2026-01-15-09\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 usage_hour_partition=2026-01-15-10\n",
      "drwxr-xr-x 1 root root 4096 Jan 25 20:25 usage_hour_partition=2026-01-15-11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify Delta tables are partitioned by usage_hour (only when VERBOSE is enabled)\n",
    "if VERBOSE:\n",
    "    import subprocess\n",
    "\n",
    "    print('=== Hourly Usage Delta Table Structure ===')\n",
    "    result = subprocess.run(['ls', '-la', 'data/output/hourly_usage/'], capture_output=True, text=True)\n",
    "    print(result.stdout)\n",
    "\n",
    "    print('\\n=== Cumulative Usage Delta Table Structure ===')\n",
    "    result = subprocess.run(['ls', '-la', 'data/output/cumulative_usage/'], capture_output=True, text=True)\n",
    "    print(result.stdout)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
